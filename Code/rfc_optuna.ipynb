{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.208461</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.791539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.481681</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.420538</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.309858</td>\n",
       "      <td>0.579462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.859182</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0.309858</td>\n",
       "      <td>0.579462</td>\n",
       "      <td>0.140818</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171162</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138697</td>\n",
       "      <td>0.579462</td>\n",
       "      <td>0.140818</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>0.315423</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.05163</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.07133</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.060713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>0.230940</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.05163</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.07133</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.060713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0.933347</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.05163</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.07133</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.060713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>0.971583</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.05163</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.07133</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.060713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>0.988404</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>0.05163</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.07133</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.060713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      2  3       100       101       102      103      104  \\\n",
       "0       0.208461   True  0  0.791539  1.000000  1.000000  1.00000  1.00000   \n",
       "1       0.481681   True  0  0.309858  1.000000  1.000000  1.00000  1.00000   \n",
       "2       0.420538   True  1  0.309858  0.579462  1.000000  1.00000  1.00000   \n",
       "3       0.859182   True  2  0.309858  0.579462  0.140818  1.00000  1.00000   \n",
       "4       0.171162   True  0  0.138697  0.579462  0.140818  1.00000  1.00000   \n",
       "...          ...    ... ..       ...       ...       ...      ...      ...   \n",
       "999995  0.315423  False -1  0.003609  0.034373  0.002928  0.00008  0.00027   \n",
       "999996  0.230940  False -1  0.003609  0.034373  0.002928  0.00008  0.00027   \n",
       "999997  0.933347  False -1  0.003609  0.034373  0.002928  0.00008  0.00027   \n",
       "999998  0.971583  False -1  0.003609  0.034373  0.002928  0.00008  0.00027   \n",
       "999999  0.988404  False -1  0.003609  0.034373  0.002928  0.00008  0.00027   \n",
       "\n",
       "             105     106  ...       110       111      112       113  \\\n",
       "0       1.000000  1.0000  ...  1.000000  1.000000  1.00000  1.000000   \n",
       "1       1.000000  1.0000  ...  1.000000  1.000000  1.00000  1.000000   \n",
       "2       1.000000  1.0000  ...  1.000000  1.000000  1.00000  1.000000   \n",
       "3       1.000000  1.0000  ...  1.000000  1.000000  1.00000  1.000000   \n",
       "4       1.000000  1.0000  ...  1.000000  1.000000  1.00000  1.000000   \n",
       "...          ...     ...  ...       ...       ...      ...       ...   \n",
       "999995  0.013412  0.0038  ...  0.023717  0.016353  0.05163  0.003678   \n",
       "999996  0.013412  0.0038  ...  0.023717  0.016353  0.05163  0.003678   \n",
       "999997  0.013412  0.0038  ...  0.023717  0.016353  0.05163  0.003678   \n",
       "999998  0.013412  0.0038  ...  0.023717  0.016353  0.05163  0.003678   \n",
       "999999  0.013412  0.0038  ...  0.023717  0.016353  0.05163  0.003678   \n",
       "\n",
       "             114       115       116      117       118       119  \n",
       "0       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
       "1       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
       "2       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
       "3       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
       "4       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
       "...          ...       ...       ...      ...       ...       ...  \n",
       "999995  0.002267  0.001295  0.024526  0.07133  0.001911  0.060713  \n",
       "999996  0.002267  0.001295  0.024526  0.07133  0.001911  0.060713  \n",
       "999997  0.002267  0.001295  0.024526  0.07133  0.001911  0.060713  \n",
       "999998  0.002267  0.001295  0.024526  0.07133  0.001911  0.060713  \n",
       "999999  0.002267  0.001295  0.024526  0.07133  0.001911  0.060713  \n",
       "\n",
       "[1000000 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = pd.read_csv('merged_data.csv')\n",
    "# pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd\n",
    "train, test = train_test_split(df, train_size=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569306    -1\n",
       "491516    -1\n",
       "312397    -1\n",
       "703051    15\n",
       "471074    -1\n",
       "          ..\n",
       "368691    -1\n",
       "219881    -1\n",
       "439609     4\n",
       "372918    13\n",
       "29836     -1\n",
       "Name: 3, Length: 900000, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['3']\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>569306</th>\n",
       "      <td>0.978245</td>\n",
       "      <td>False</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.045645</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.066166</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>0.009225</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.068118</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.035059</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.131350</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.034323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491516</th>\n",
       "      <td>0.379379</td>\n",
       "      <td>False</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.042008</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>0.042550</td>\n",
       "      <td>0.015512</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.037645</td>\n",
       "      <td>0.099630</td>\n",
       "      <td>0.008239</td>\n",
       "      <td>0.014596</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.018142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312397</th>\n",
       "      <td>0.232445</td>\n",
       "      <td>False</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.099413</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.003053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703051</th>\n",
       "      <td>0.015988</td>\n",
       "      <td>True</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.069461</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.025606</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.063027</td>\n",
       "      <td>0.148645</td>\n",
       "      <td>0.155236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165769</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.069118</td>\n",
       "      <td>0.125893</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.076956</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>0.081551</td>\n",
       "      <td>0.057945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471074</th>\n",
       "      <td>0.685894</td>\n",
       "      <td>False</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.154851</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.034412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.029446</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.023595</td>\n",
       "      <td>0.121204</td>\n",
       "      <td>0.127139</td>\n",
       "      <td>0.063337</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.125455</td>\n",
       "      <td>0.035128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368691</th>\n",
       "      <td>0.397401</td>\n",
       "      <td>False</td>\n",
       "      <td>0.085640</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.063746</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.021353</td>\n",
       "      <td>0.025255</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.053459</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.049933</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.083519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219881</th>\n",
       "      <td>0.836784</td>\n",
       "      <td>False</td>\n",
       "      <td>0.076796</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.101473</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.105020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094054</td>\n",
       "      <td>0.028945</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.133015</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.149029</td>\n",
       "      <td>0.089291</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.022121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439609</th>\n",
       "      <td>0.340150</td>\n",
       "      <td>True</td>\n",
       "      <td>0.108291</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.157066</td>\n",
       "      <td>0.068971</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372918</th>\n",
       "      <td>0.013624</td>\n",
       "      <td>True</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.051375</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>0.041274</td>\n",
       "      <td>0.051416</td>\n",
       "      <td>0.014105</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.046642</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.055807</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.093617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29836</th>\n",
       "      <td>0.927578</td>\n",
       "      <td>False</td>\n",
       "      <td>0.154634</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.072074</td>\n",
       "      <td>0.173859</td>\n",
       "      <td>0.034752</td>\n",
       "      <td>0.025028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.157263</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.035518</td>\n",
       "      <td>0.087734</td>\n",
       "      <td>0.397191</td>\n",
       "      <td>0.496412</td>\n",
       "      <td>0.055644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      2       100       101       102       103       104  \\\n",
       "569306  0.978245  False  0.004921  0.019557  0.045645  0.008605  0.003419   \n",
       "491516  0.379379  False  0.015239  0.042008  0.004030  0.005652  0.042550   \n",
       "312397  0.232445  False  0.010208  0.004356  0.017959  0.000048  0.099413   \n",
       "703051  0.015988   True  0.019704  0.069461  0.018286  0.025606  0.014371   \n",
       "471074  0.685894  False  0.021372  0.000743  0.021346  0.002038  0.154851   \n",
       "...          ...    ...       ...       ...       ...       ...       ...   \n",
       "368691  0.397401  False  0.085640  0.052196  0.004527  0.063746  0.028512   \n",
       "219881  0.836784  False  0.076796  0.003463  0.004802  0.011859  0.000710   \n",
       "439609  0.340150   True  0.108291  0.050264  0.157066  0.068971  0.020255   \n",
       "372918  0.013624   True  0.010969  0.006748  0.012277  0.051375  0.000697   \n",
       "29836   0.927578  False  0.154634  0.001998  0.015846  0.003768  0.072074   \n",
       "\n",
       "             105       106       107  ...       110       111       112  \\\n",
       "569306  0.068182  0.066166  0.000340  ...  0.016255  0.009225  0.003519   \n",
       "491516  0.015512  0.001964  0.003924  ...  0.000634  0.000099  0.003106   \n",
       "312397  0.000165  0.000818  0.005214  ...  0.003714  0.011692  0.022512   \n",
       "703051  0.063027  0.148645  0.155236  ...  0.165769  0.001467  0.069118   \n",
       "471074  0.021641  0.004572  0.034412  ...  0.006238  0.029446  0.003735   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "368691  0.012022  0.003518  0.000574  ...  0.016181  0.007676  0.021353   \n",
       "219881  0.101473  0.011463  0.105020  ...  0.094054  0.028945  0.007608   \n",
       "439609  1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "372918  0.001090  0.008504  0.007940  ...  0.004262  0.041274  0.051416   \n",
       "29836   0.173859  0.034752  0.025028  ...  0.004618  0.152250  0.009407   \n",
       "\n",
       "             113       114       115       116       117       118       119  \n",
       "569306  0.068118  0.006803  0.035059  0.013839  0.131350  0.001915  0.034323  \n",
       "491516  0.015671  0.037645  0.099630  0.008239  0.014596  0.004089  0.018142  \n",
       "312397  0.021989  0.003186  0.010685  0.004753  0.001828  0.011843  0.003053  \n",
       "703051  0.125893  0.010060  0.000483  0.076956  0.019115  0.081551  0.057945  \n",
       "471074  0.023595  0.121204  0.127139  0.063337  0.026104  0.125455  0.035128  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "368691  0.025255  0.020806  0.053459  0.004054  0.049933  0.006605  0.083519  \n",
       "219881  0.133015  0.005553  0.149029  0.089291  0.005854  0.024882  0.022121  \n",
       "439609  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "372918  0.014105  0.008287  0.046642  0.007009  0.055807  0.000604  0.093617  \n",
       "29836   0.157263  0.040022  0.035518  0.087734  0.397191  0.496412  0.055644  \n",
       "\n",
       "[900000 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train.drop(['3'], axis='columns')\n",
    "\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210115     3\n",
       "560254    -1\n",
       "338687    -1\n",
       "784117    -1\n",
       "108040    -1\n",
       "          ..\n",
       "339346    -1\n",
       "132810     4\n",
       "67723     -1\n",
       "909628    10\n",
       "887104    -1\n",
       "Name: 3, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test['3']\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210115</th>\n",
       "      <td>0.022732</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.016621</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>0.072778</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.017821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560254</th>\n",
       "      <td>0.260008</td>\n",
       "      <td>False</td>\n",
       "      <td>0.074598</td>\n",
       "      <td>0.089310</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.074979</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.150510</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>0.154439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075318</td>\n",
       "      <td>0.094144</td>\n",
       "      <td>0.087093</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>0.027088</td>\n",
       "      <td>0.012413</td>\n",
       "      <td>0.059066</td>\n",
       "      <td>0.065134</td>\n",
       "      <td>0.057554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338687</th>\n",
       "      <td>0.819829</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>0.013741</td>\n",
       "      <td>0.085927</td>\n",
       "      <td>0.046933</td>\n",
       "      <td>0.052078</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.021206</td>\n",
       "      <td>0.020987</td>\n",
       "      <td>0.081311</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.058368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784117</th>\n",
       "      <td>0.475077</td>\n",
       "      <td>False</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.047754</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.016403</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.026133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108040</th>\n",
       "      <td>0.353520</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.233875</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.015803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>0.251335</td>\n",
       "      <td>0.039990</td>\n",
       "      <td>0.048236</td>\n",
       "      <td>0.352780</td>\n",
       "      <td>0.048981</td>\n",
       "      <td>0.056274</td>\n",
       "      <td>0.146391</td>\n",
       "      <td>0.086751</td>\n",
       "      <td>0.257410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339346</th>\n",
       "      <td>0.221482</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.026407</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.018917</td>\n",
       "      <td>0.018214</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>0.014702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132810</th>\n",
       "      <td>0.561646</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100087</td>\n",
       "      <td>0.083384</td>\n",
       "      <td>0.053099</td>\n",
       "      <td>0.148220</td>\n",
       "      <td>0.438354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67723</th>\n",
       "      <td>0.323348</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.125812</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.122030</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015684</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.035134</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.019241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909628</th>\n",
       "      <td>0.378525</td>\n",
       "      <td>True</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.112995</td>\n",
       "      <td>0.110648</td>\n",
       "      <td>0.019239</td>\n",
       "      <td>0.024948</td>\n",
       "      <td>0.137743</td>\n",
       "      <td>0.090727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887104</th>\n",
       "      <td>0.399713</td>\n",
       "      <td>False</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.120052</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.012846</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>0.023141</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      2       100       101       102       103       104  \\\n",
       "210115  0.022732   True  0.003846  0.013557  0.001995  0.016488  0.004917   \n",
       "560254  0.260008  False  0.074598  0.089310  0.080078  0.074979  0.004046   \n",
       "338687  0.819829  False  0.014056  0.013741  0.085927  0.046933  0.052078   \n",
       "784117  0.475077  False  0.033227  0.002004  0.000168  0.047754  0.023887   \n",
       "108040  0.353520  False  0.011935  0.000917  0.025480  0.018286  0.233875   \n",
       "...          ...    ...       ...       ...       ...       ...       ...   \n",
       "339346  0.221482  False  0.008126  0.010802  0.017457  0.017058  0.001969   \n",
       "132810  0.561646   True  0.100087  0.083384  0.053099  0.148220  0.438354   \n",
       "67723   0.323348  False  0.000621  0.125812  0.015420  0.000372  0.009014   \n",
       "909628  0.378525   True  0.067870  0.005626  0.112995  0.110648  0.019239   \n",
       "887104  0.399713  False  0.026503  0.024523  0.022532  0.005294  0.005281   \n",
       "\n",
       "             105       106       107  ...       110       111       112  \\\n",
       "210115  0.000345  0.004589  0.012041  ...  0.015146  0.049615  0.004454   \n",
       "560254  0.150510  0.010796  0.154439  ...  0.075318  0.094144  0.087093   \n",
       "338687  0.004111  0.006440  0.003827  ...  0.054081  0.017050  0.016287   \n",
       "784117  0.007342  0.000567  0.016565  ...  0.005148  0.007565  0.001829   \n",
       "108040  0.005297  0.001608  0.015803  ...  0.138639  0.251335  0.039990   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "339346  0.026766  0.026407  0.001757  ...  0.000234  0.018917  0.018214   \n",
       "132810  1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "67723   0.000262  0.122030  0.001284  ...  0.015684  0.008844  0.002999   \n",
       "909628  0.024948  0.137743  0.090727  ...  0.166547  1.000000  1.000000   \n",
       "887104  0.000956  0.004397  0.000107  ...  0.011565  0.120052  0.015550   \n",
       "\n",
       "             113       114       115       116       117       118       119  \n",
       "210115  0.016621  0.003672  0.008548  0.072778  0.019998  0.013537  0.017821  \n",
       "560254  0.008220  0.055004  0.027088  0.012413  0.059066  0.065134  0.057554  \n",
       "338687  0.021206  0.020987  0.081311  0.000056  0.004294  0.002105  0.058368  \n",
       "784117  0.001830  0.002909  0.008337  0.016403  0.020595  0.000728  0.026133  \n",
       "108040  0.048236  0.352780  0.048981  0.056274  0.146391  0.086751  0.257410  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "339346  0.001847  0.012308  0.021123  0.007064  0.010102  0.006499  0.014702  \n",
       "132810  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "67723   0.012550  0.009978  0.035134  0.012003  0.004405  0.005739  0.019241  \n",
       "909628  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "887104  0.013292  0.011670  0.005088  0.012846  0.024384  0.023141  0.003478  \n",
       "\n",
       "[100000 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test.drop(['3'], axis='columns')\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['2'], axis='columns')\n",
    "x_train.drop(columns='3')\n",
    "# y_train\n",
    "\n",
    "x_test.drop(columns='2', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-01 23:19:01,074]\u001b[0m A new study created in memory with name: no-name-d5a6425e-3eb8-40d2-8b64-c01d6480f057\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:12,270]\u001b[0m Trial 0 finished with value: 0.84422 and parameters: {'n_estimators': 15, 'max_depth': 15.29244346763837}. Best is trial 0 with value: 0.84422.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:14,847]\u001b[0m Trial 1 finished with value: 0.83986 and parameters: {'n_estimators': 4, 'max_depth': 10.861117465263552}. Best is trial 0 with value: 0.84422.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:22,673]\u001b[0m Trial 2 finished with value: 0.84445 and parameters: {'n_estimators': 12, 'max_depth': 14.37612850399151}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:23,451]\u001b[0m Trial 3 finished with value: 0.7365 and parameters: {'n_estimators': 5, 'max_depth': 1.1942207973874717}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:42,782]\u001b[0m Trial 4 finished with value: 0.84429 and parameters: {'n_estimators': 19, 'max_depth': 24.86167781085082}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:44,100]\u001b[0m Trial 5 finished with value: 0.73615 and parameters: {'n_estimators': 8, 'max_depth': 1.305758445836046}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:51,282]\u001b[0m Trial 6 finished with value: 0.83917 and parameters: {'n_estimators': 12, 'max_depth': 10.272234354892428}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:19:54,704]\u001b[0m Trial 7 finished with value: 0.75511 and parameters: {'n_estimators': 17, 'max_depth': 2.8866723641775818}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:20:04,642]\u001b[0m Trial 8 finished with value: 0.83843 and parameters: {'n_estimators': 18, 'max_depth': 10.428987499202513}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:20:19,484]\u001b[0m Trial 9 finished with value: 0.84419 and parameters: {'n_estimators': 14, 'max_depth': 22.69434584972308}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:20:22,165]\u001b[0m Trial 10 finished with value: 0.82957 and parameters: {'n_estimators': 9, 'max_depth': 4.93989654014128}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:20:45,153]\u001b[0m Trial 11 finished with value: 0.84161 and parameters: {'n_estimators': 20, 'max_depth': 31.507177630666273}. Best is trial 2 with value: 0.84445.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:00,975]\u001b[0m Trial 12 finished with value: 0.84549 and parameters: {'n_estimators': 20, 'max_depth': 19.716928128263966}. Best is trial 12 with value: 0.84549.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:03,873]\u001b[0m Trial 13 finished with value: 0.83512 and parameters: {'n_estimators': 2, 'max_depth': 17.19169023086607}. Best is trial 12 with value: 0.84549.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:07,767]\u001b[0m Trial 14 finished with value: 0.83624 and parameters: {'n_estimators': 11, 'max_depth': 6.023559839689395}. Best is trial 12 with value: 0.84549.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:27,109]\u001b[0m Trial 15 finished with value: 0.84105 and parameters: {'n_estimators': 15, 'max_depth': 31.888593970335485}. Best is trial 12 with value: 0.84549.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:30,039]\u001b[0m Trial 16 finished with value: 0.8339 and parameters: {'n_estimators': 8, 'max_depth': 6.87839795369212}. Best is trial 12 with value: 0.84549.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:40,823]\u001b[0m Trial 17 finished with value: 0.84576 and parameters: {'n_estimators': 13, 'max_depth': 16.94780324970114}. Best is trial 17 with value: 0.84576.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:21:45,412]\u001b[0m Trial 18 finished with value: 0.82947 and parameters: {'n_estimators': 17, 'max_depth': 4.159409963614113}. Best is trial 17 with value: 0.84576.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:02,900]\u001b[0m Trial 19 finished with value: 0.84418 and parameters: {'n_estimators': 20, 'max_depth': 22.32792715913317}. Best is trial 17 with value: 0.84576.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:05,438]\u001b[0m Trial 20 finished with value: 0.75036 and parameters: {'n_estimators': 14, 'max_depth': 2.097454377835461}. Best is trial 17 with value: 0.84576.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:13,682]\u001b[0m Trial 21 finished with value: 0.84484 and parameters: {'n_estimators': 12, 'max_depth': 15.761368533062829}. Best is trial 17 with value: 0.84576.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:24,610]\u001b[0m Trial 22 finished with value: 0.84595 and parameters: {'n_estimators': 13, 'max_depth': 17.06101551008792}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:28,790]\u001b[0m Trial 23 finished with value: 0.83764 and parameters: {'n_estimators': 9, 'max_depth': 9.150470889984371}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:42,816]\u001b[0m Trial 24 finished with value: 0.84443 and parameters: {'n_estimators': 16, 'max_depth': 20.258283497020457}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:22:58,983]\u001b[0m Trial 25 finished with value: 0.84144 and parameters: {'n_estimators': 14, 'max_depth': 28.60999767017735}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:01,500]\u001b[0m Trial 26 finished with value: 0.83708 and parameters: {'n_estimators': 6, 'max_depth': 7.931576402215552}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:09,849]\u001b[0m Trial 27 finished with value: 0.84157 and parameters: {'n_estimators': 13, 'max_depth': 12.607776487614341}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:17,552]\u001b[0m Trial 28 finished with value: 0.84468 and parameters: {'n_estimators': 10, 'max_depth': 18.58181589448748}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:27,670]\u001b[0m Trial 29 finished with value: 0.84326 and parameters: {'n_estimators': 16, 'max_depth': 13.5440116431804}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:46,285]\u001b[0m Trial 30 finished with value: 0.84269 and parameters: {'n_estimators': 18, 'max_depth': 26.897062786606966}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:23:54,906]\u001b[0m Trial 31 finished with value: 0.84494 and parameters: {'n_estimators': 12, 'max_depth': 16.931534260765545}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:02,274]\u001b[0m Trial 32 finished with value: 0.84408 and parameters: {'n_estimators': 10, 'max_depth': 17.76436151645188}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:10,642]\u001b[0m Trial 33 finished with value: 0.84229 and parameters: {'n_estimators': 13, 'max_depth': 12.48849928085728}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:18,043]\u001b[0m Trial 34 finished with value: 0.84486 and parameters: {'n_estimators': 11, 'max_depth': 14.303366397784453}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:24,784]\u001b[0m Trial 35 finished with value: 0.83694 and parameters: {'n_estimators': 13, 'max_depth': 8.526202947750019}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:31,310]\u001b[0m Trial 36 finished with value: 0.83991 and parameters: {'n_estimators': 6, 'max_depth': 24.335788296660642}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:38,834]\u001b[0m Trial 37 finished with value: 0.84107 and parameters: {'n_estimators': 12, 'max_depth': 12.063078991320532}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:49,812]\u001b[0m Trial 38 finished with value: 0.84273 and parameters: {'n_estimators': 15, 'max_depth': 15.742621865090973}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:53,816]\u001b[0m Trial 39 finished with value: 0.83897 and parameters: {'n_estimators': 3, 'max_depth': 20.144169293168446}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:24:57,499]\u001b[0m Trial 40 finished with value: 0.83821 and parameters: {'n_estimators': 7, 'max_depth': 10.38859160923142}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:04,696]\u001b[0m Trial 41 finished with value: 0.84559 and parameters: {'n_estimators': 11, 'max_depth': 14.801546353478622}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:13,969]\u001b[0m Trial 42 finished with value: 0.8435 and parameters: {'n_estimators': 11, 'max_depth': 20.428728204278457}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:20,821]\u001b[0m Trial 43 finished with value: 0.8451 and parameters: {'n_estimators': 10, 'max_depth': 15.402569405210745}. Best is trial 22 with value: 0.84595.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-01 23:25:30,507]\u001b[0m Trial 44 finished with value: 0.84128 and parameters: {'n_estimators': 10, 'max_depth': 25.109471590938846}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:36,515]\u001b[0m Trial 45 finished with value: 0.84169 and parameters: {'n_estimators': 9, 'max_depth': 14.653345297097596}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:40,903]\u001b[0m Trial 46 finished with value: 0.84133 and parameters: {'n_estimators': 8, 'max_depth': 11.189989375147878}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:52,313]\u001b[0m Trial 47 finished with value: 0.8406 and parameters: {'n_estimators': 10, 'max_depth': 31.08548240415054}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:25:59,208]\u001b[0m Trial 48 finished with value: 0.83776 and parameters: {'n_estimators': 13, 'max_depth': 9.500980308044783}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:13,926]\u001b[0m Trial 49 finished with value: 0.84339 and parameters: {'n_estimators': 15, 'max_depth': 23.308038624819236}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:18,224]\u001b[0m Trial 50 finished with value: 0.83666 and parameters: {'n_estimators': 11, 'max_depth': 7.117726650888948}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:24,268]\u001b[0m Trial 51 finished with value: 0.84466 and parameters: {'n_estimators': 9, 'max_depth': 15.960459930882228}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:33,538]\u001b[0m Trial 52 finished with value: 0.84414 and parameters: {'n_estimators': 12, 'max_depth': 18.639054212633788}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:44,469]\u001b[0m Trial 53 finished with value: 0.84494 and parameters: {'n_estimators': 14, 'max_depth': 16.87112122212688}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:26:58,020]\u001b[0m Trial 54 finished with value: 0.84443 and parameters: {'n_estimators': 14, 'max_depth': 21.21130115196952}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:27:05,409]\u001b[0m Trial 55 finished with value: 0.8433 and parameters: {'n_estimators': 12, 'max_depth': 13.581813060428248}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:27:13,591]\u001b[0m Trial 56 finished with value: 0.83974 and parameters: {'n_estimators': 14, 'max_depth': 11.204819876568264}. Best is trial 22 with value: 0.84595.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:27:26,601]\u001b[0m Trial 57 finished with value: 0.84642 and parameters: {'n_estimators': 16, 'max_depth': 18.752647998968612}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:27:46,097]\u001b[0m Trial 58 finished with value: 0.84258 and parameters: {'n_estimators': 19, 'max_depth': 27.100319504984174}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:27:50,440]\u001b[0m Trial 59 finished with value: 0.82155 and parameters: {'n_estimators': 16, 'max_depth': 4.847964912181106}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:28:10,272]\u001b[0m Trial 60 finished with value: 0.84163 and parameters: {'n_estimators': 18, 'max_depth': 29.476793961170685}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:28:19,610]\u001b[0m Trial 61 finished with value: 0.84435 and parameters: {'n_estimators': 12, 'max_depth': 18.375251173346005}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:28:30,540]\u001b[0m Trial 62 finished with value: 0.84467 and parameters: {'n_estimators': 15, 'max_depth': 15.094387320312784}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:28:43,971]\u001b[0m Trial 63 finished with value: 0.84399 and parameters: {'n_estimators': 13, 'max_depth': 22.02507201798207}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:28:57,829]\u001b[0m Trial 64 finished with value: 0.84584 and parameters: {'n_estimators': 17, 'max_depth': 19.028705216381944}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:29:11,821]\u001b[0m Trial 65 finished with value: 0.84595 and parameters: {'n_estimators': 17, 'max_depth': 19.991510333863435}. Best is trial 57 with value: 0.84642.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:29:25,364]\u001b[0m Trial 66 finished with value: 0.84643 and parameters: {'n_estimators': 17, 'max_depth': 18.980392203476523}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:29:42,003]\u001b[0m Trial 67 finished with value: 0.84362 and parameters: {'n_estimators': 17, 'max_depth': 24.847285742316856}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:29:55,514]\u001b[0m Trial 68 finished with value: 0.84574 and parameters: {'n_estimators': 17, 'max_depth': 18.943901667313604}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:30:10,717]\u001b[0m Trial 69 finished with value: 0.84596 and parameters: {'n_estimators': 19, 'max_depth': 19.231025934457655}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:30:13,039]\u001b[0m Trial 70 finished with value: 0.73628 and parameters: {'n_estimators': 19, 'max_depth': 1.4268386248721059}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:30:28,776]\u001b[0m Trial 71 finished with value: 0.84458 and parameters: {'n_estimators': 18, 'max_depth': 21.613251594608645}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:30:42,316]\u001b[0m Trial 72 finished with value: 0.84551 and parameters: {'n_estimators': 17, 'max_depth': 18.714871308901962}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:30:51,909]\u001b[0m Trial 73 finished with value: 0.8413 and parameters: {'n_estimators': 16, 'max_depth': 12.979511080525787}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:31:05,339]\u001b[0m Trial 74 finished with value: 0.84497 and parameters: {'n_estimators': 18, 'max_depth': 17.742018412755513}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:31:22,884]\u001b[0m Trial 75 finished with value: 0.84378 and parameters: {'n_estimators': 19, 'max_depth': 23.40913064023452}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:31:40,324]\u001b[0m Trial 76 finished with value: 0.84239 and parameters: {'n_estimators': 17, 'max_depth': 26.01869183972355}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:31:56,074]\u001b[0m Trial 77 finished with value: 0.84586 and parameters: {'n_estimators': 20, 'max_depth': 19.931241774799886}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:32:16,351]\u001b[0m Trial 78 finished with value: 0.84213 and parameters: {'n_estimators': 20, 'max_depth': 27.916114231916076}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:32:30,344]\u001b[0m Trial 79 finished with value: 0.84632 and parameters: {'n_estimators': 20, 'max_depth': 16.556918509206344}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:32:47,461]\u001b[0m Trial 80 finished with value: 0.84473 and parameters: {'n_estimators': 20, 'max_depth': 21.196645333950425}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:33:00,837]\u001b[0m Trial 81 finished with value: 0.84552 and parameters: {'n_estimators': 19, 'max_depth': 16.988551050025706}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:33:14,758]\u001b[0m Trial 82 finished with value: 0.84481 and parameters: {'n_estimators': 16, 'max_depth': 20.082820690583855}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:33:27,710]\u001b[0m Trial 83 finished with value: 0.84483 and parameters: {'n_estimators': 18, 'max_depth': 16.519824596022413}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:33:39,571]\u001b[0m Trial 84 finished with value: 0.8414 and parameters: {'n_estimators': 20, 'max_depth': 13.75172021388622}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:33:49,752]\u001b[0m Trial 85 finished with value: 0.8393 and parameters: {'n_estimators': 19, 'max_depth': 11.713286596335484}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:34:05,024]\u001b[0m Trial 86 finished with value: 0.84402 and parameters: {'n_estimators': 16, 'max_depth': 23.584904664515204}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:34:18,963]\u001b[0m Trial 87 finished with value: 0.84557 and parameters: {'n_estimators': 17, 'max_depth': 19.291727365464165}. Best is trial 66 with value: 0.84643.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-01 23:34:22,169]\u001b[0m Trial 88 finished with value: 0.75744 and parameters: {'n_estimators': 18, 'max_depth': 2.887610028174072}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:34:41,413]\u001b[0m Trial 89 finished with value: 0.84122 and parameters: {'n_estimators': 15, 'max_depth': 31.05491882455168}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:34:55,785]\u001b[0m Trial 90 finished with value: 0.84393 and parameters: {'n_estimators': 20, 'max_depth': 15.73246498627502}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:35:11,028]\u001b[0m Trial 91 finished with value: 0.84503 and parameters: {'n_estimators': 17, 'max_depth': 19.17096293543844}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:35:25,768]\u001b[0m Trial 92 finished with value: 0.84633 and parameters: {'n_estimators': 19, 'max_depth': 17.422234030195764}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:35:39,701]\u001b[0m Trial 93 finished with value: 0.84587 and parameters: {'n_estimators': 19, 'max_depth': 17.253125905274636}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:35:57,271]\u001b[0m Trial 94 finished with value: 0.84492 and parameters: {'n_estimators': 19, 'max_depth': 22.474334712457985}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:36:12,462]\u001b[0m Trial 95 finished with value: 0.84475 and parameters: {'n_estimators': 20, 'max_depth': 17.365554555890313}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:36:24,522]\u001b[0m Trial 96 finished with value: 0.84319 and parameters: {'n_estimators': 19, 'max_depth': 14.57762841491168}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:36:34,892]\u001b[0m Trial 97 finished with value: 0.84076 and parameters: {'n_estimators': 18, 'max_depth': 12.976812979156124}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:36:49,943]\u001b[0m Trial 98 finished with value: 0.84456 and parameters: {'n_estimators': 18, 'max_depth': 20.64849889128452}. Best is trial 66 with value: 0.84643.\u001b[0m\n",
      "\u001b[32m[I 2021-04-01 23:37:08,614]\u001b[0m Trial 99 finished with value: 0.84238 and parameters: {'n_estimators': 19, 'max_depth': 25.556014297880978}. Best is trial 66 with value: 0.84643.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84643\n",
      "Best hyperparameters: {'n_estimators': 17, 'max_depth': 18.980392203476523}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
    "    max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True))\n",
    "    \n",
    "    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1)\n",
    "    \n",
    "    clf.fit(x_train.drop(columns='3'), y_train)\n",
    "    \n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
